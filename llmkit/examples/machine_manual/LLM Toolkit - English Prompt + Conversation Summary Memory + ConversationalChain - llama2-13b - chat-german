{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FZ6Xovx6vOLE0TatA12cfT-Dh_-TYxoD","timestamp":1701083734385},{"file_id":"1tL9nRi-0j4MypyaUjDSiue-YibVpaXHQ","timestamp":1701079348910},{"file_id":"16lYsnfKr-x3TG7iU1IV-Y8nZlyc_NgZZ","timestamp":1700735356047},{"file_id":"1pva39kHCzJFxOI9c1CW_4tcxZJJy3qyo","timestamp":1700729458945},{"file_id":"18zM4SQg5qbtsb0K2x4skob5nU2MlpEmm","timestamp":1700638336539},{"file_id":"14fjYCt48f79SYKZELqUFWdlddwoJc7Ju","timestamp":1697713892289},{"file_id":"1PdWY57nGk4EB8bRM3t05EjR8ab_MbxH1","timestamp":1696602099329},{"file_id":"177CVK3nuHTP8mwEDe0oy50UxUKSSpRRv","timestamp":1694695391509},{"file_id":"1DY_WLsFWy4zf0IBs9S1k8qc1JwTgfyFF","timestamp":1694624577197},{"file_id":"1bARZZnxF8lYBxtPGYFtMD7Rfc9-ckbzz","timestamp":1689157906094},{"file_id":"1JwzfVRZMEUSRhZZldDZrz5s-MyjkGn-i","timestamp":1688544410653}],"gpuType":"V100","machine_shape":"hm","collapsed_sections":["jxUrrRCHu6Fk"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ee2403a5567c48be8c47c9f5073b9778":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_baa9fcf56dfb4559b75508b2ebc13e42","IPY_MODEL_5980ad47c8b34e4a98872695d8a35ba5","IPY_MODEL_8fbe6c606f7b4dee9c594e76ca12541b"],"layout":"IPY_MODEL_71689dcee48649b1af58b0dc830275b3"}},"baa9fcf56dfb4559b75508b2ebc13e42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41201c718c8b4775a1b8abb590e9ce4c","placeholder":"​","style":"IPY_MODEL_deffb3d2946642e19f02b146ea709aec","value":"Loading checkpoint shards: 100%"}},"5980ad47c8b34e4a98872695d8a35ba5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_950d26323d2e41a7aac9c700fa5d6821","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d9378757bb140fb9feaed29055b711c","value":3}},"8fbe6c606f7b4dee9c594e76ca12541b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4c68f68b8e14894ba4b142c097d9b43","placeholder":"​","style":"IPY_MODEL_64b1d3031ff840dbbadcb6d04c12bb7a","value":" 3/3 [04:40&lt;00:00, 88.58s/it]"}},"71689dcee48649b1af58b0dc830275b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41201c718c8b4775a1b8abb590e9ce4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deffb3d2946642e19f02b146ea709aec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"950d26323d2e41a7aac9c700fa5d6821":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d9378757bb140fb9feaed29055b711c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4c68f68b8e14894ba4b142c097d9b43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64b1d3031ff840dbbadcb6d04c12bb7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ef43c9292364bb9b7897690815fdfd3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3274cc318c048e88205ab9bbc7f750c","IPY_MODEL_cf289ebc7dc44d9cbb112789f7d6c8ff","IPY_MODEL_b93f75562017477eb33ac1f68e015405"],"layout":"IPY_MODEL_64e9d009bf864ea294f2317e3a00470c"}},"b3274cc318c048e88205ab9bbc7f750c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51f839df7d9449aaae63aa5914cfd521","placeholder":"​","style":"IPY_MODEL_45d1dcdb0d484192aee67ea7137922e1","value":".gitattributes: 100%"}},"cf289ebc7dc44d9cbb112789f7d6c8ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_894a074634d14291aa2920766b0b52fa","max":496,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61d3c0e2760b4f6d9b13184245818857","value":496}},"b93f75562017477eb33ac1f68e015405":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cc1c44f071c4ccdbbccd06f7e2d8086","placeholder":"​","style":"IPY_MODEL_2d25385b096e49ca906eadd8b93f8b6c","value":" 496/496 [00:00&lt;00:00, 45.7kB/s]"}},"64e9d009bf864ea294f2317e3a00470c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51f839df7d9449aaae63aa5914cfd521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45d1dcdb0d484192aee67ea7137922e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"894a074634d14291aa2920766b0b52fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61d3c0e2760b4f6d9b13184245818857":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0cc1c44f071c4ccdbbccd06f7e2d8086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d25385b096e49ca906eadd8b93f8b6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1fe43a063ea24e7e9fb10b44357fcf08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4235128547b14435af39d5371948df50","IPY_MODEL_8b218290f81a404bb52308afe1af1ee5","IPY_MODEL_3985b79764524e4fb8ee9d59f734ed5a"],"layout":"IPY_MODEL_67a0b7101c584f928a1f069a841c83d5"}},"4235128547b14435af39d5371948df50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_877db5398c124ec1a9dc92f2c9a3c988","placeholder":"​","style":"IPY_MODEL_1e43616097504bcb81ad2001b77a71af","value":"README.md: 100%"}},"8b218290f81a404bb52308afe1af1ee5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_97923982ba5c46b0b46b19537c2fe91c","max":4412,"min":0,"orientation":"horizontal","style":"IPY_MODEL_474c14ce597b4f8b984e42ef468e90c4","value":4412}},"3985b79764524e4fb8ee9d59f734ed5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0664e306fea54a6fae7baa3beda86691","placeholder":"​","style":"IPY_MODEL_b568cfe2470c46c299796239a37bd6fd","value":" 4.41k/4.41k [00:00&lt;00:00, 378kB/s]"}},"67a0b7101c584f928a1f069a841c83d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"877db5398c124ec1a9dc92f2c9a3c988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e43616097504bcb81ad2001b77a71af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97923982ba5c46b0b46b19537c2fe91c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"474c14ce597b4f8b984e42ef468e90c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0664e306fea54a6fae7baa3beda86691":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b568cfe2470c46c299796239a37bd6fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c0f13ff27114ea08e376746c4fda4fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf83e4d45beb4e7db3c7a284e541153b","IPY_MODEL_b1e17b91c6f84c989476aaccc19d4bd0","IPY_MODEL_7c00db19a9284cb2bd9cc231218f7bf9"],"layout":"IPY_MODEL_894047e08a9345179ae0e0c33c909ba9"}},"bf83e4d45beb4e7db3c7a284e541153b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b0b07fcb42845079557b61dd83db4d5","placeholder":"​","style":"IPY_MODEL_64790a4f3def47eb86242f5482a6f9b2","value":"config.json: 100%"}},"b1e17b91c6f84c989476aaccc19d4bd0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7748662039314055b53496ec90242ea9","max":611,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c39e656d1c0446baa6f4092c5645da25","value":611}},"7c00db19a9284cb2bd9cc231218f7bf9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f72ac175581a424bb829c2137f6bd729","placeholder":"​","style":"IPY_MODEL_f74f535b35934fe281830a71174d3da0","value":" 611/611 [00:00&lt;00:00, 59.0kB/s]"}},"894047e08a9345179ae0e0c33c909ba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b0b07fcb42845079557b61dd83db4d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64790a4f3def47eb86242f5482a6f9b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7748662039314055b53496ec90242ea9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c39e656d1c0446baa6f4092c5645da25":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f72ac175581a424bb829c2137f6bd729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f74f535b35934fe281830a71174d3da0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29e5014988034fac829421f0a28e6bcd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36f8f48d83464a5983525dec6822564c","IPY_MODEL_887e97210e344388ac00b6bcc8be2909","IPY_MODEL_e3f94a37bbd14bbfbd955d826f6bb5c0"],"layout":"IPY_MODEL_0785a08a1d9e415c807f55502cdd9eb0"}},"36f8f48d83464a5983525dec6822564c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83ea489e3ed648a888df0ef935d1ad83","placeholder":"​","style":"IPY_MODEL_727b68f80c8d47ad8b0221c90ab254dd","value":"model.safetensors: 100%"}},"887e97210e344388ac00b6bcc8be2909":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca40565293434479900ae816f8edcf74","max":1883734344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_34b5aad4a1724c6593539410158d249b","value":1883734344}},"e3f94a37bbd14bbfbd955d826f6bb5c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c929f26c81446ef8d1c2967f51d7e75","placeholder":"​","style":"IPY_MODEL_7a2071d9cd2c4796808c52a4a419d9a9","value":" 1.88G/1.88G [00:05&lt;00:00, 343MB/s]"}},"0785a08a1d9e415c807f55502cdd9eb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83ea489e3ed648a888df0ef935d1ad83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"727b68f80c8d47ad8b0221c90ab254dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca40565293434479900ae816f8edcf74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34b5aad4a1724c6593539410158d249b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c929f26c81446ef8d1c2967f51d7e75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a2071d9cd2c4796808c52a4a419d9a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ef6f570f03f4064bb9f4bfe96b0f2eb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_059a274c14f54643b7b1f23098e23fef","IPY_MODEL_12a78cc1f6ef4037b985c19a1ea61822","IPY_MODEL_be261006453f4e76afb449948901e760"],"layout":"IPY_MODEL_0cd5f27cecae497cad778342d789afe1"}},"059a274c14f54643b7b1f23098e23fef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40b050d5fefa45359df7d2d7d74578d0","placeholder":"​","style":"IPY_MODEL_c9112162ab6f4ff8970fe4aa913d7256","value":"pytorch_model.bin: 100%"}},"12a78cc1f6ef4037b985c19a1ea61822":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68c1ce029d384d5a87803637ec89b880","max":1883775789,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c8d2db81c87d4d2391327aac0477e53e","value":1883775789}},"be261006453f4e76afb449948901e760":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab502759045c48b18f92d72bf9c14925","placeholder":"​","style":"IPY_MODEL_058cec556a014c3b990f56cb7c13b77e","value":" 1.88G/1.88G [00:04&lt;00:00, 304MB/s]"}},"0cd5f27cecae497cad778342d789afe1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40b050d5fefa45359df7d2d7d74578d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9112162ab6f4ff8970fe4aa913d7256":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68c1ce029d384d5a87803637ec89b880":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8d2db81c87d4d2391327aac0477e53e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab502759045c48b18f92d72bf9c14925":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"058cec556a014c3b990f56cb7c13b77e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3fba5b84bbdb4c19bde36812e314bc8b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7270b41e3c94f51b56537fd4ba54c30","IPY_MODEL_e35337e7a2844a5e8b294e94a971ab2e","IPY_MODEL_7d2cbedff1504352894e9edfc903944a"],"layout":"IPY_MODEL_c78a646392944f8cbe182e5f8d84728f"}},"b7270b41e3c94f51b56537fd4ba54c30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0800568090a4a23b3864b5f25f255fc","placeholder":"​","style":"IPY_MODEL_ee5ff464ee224a7692092448dc2dc7dd","value":"special_tokens_map.json: 100%"}},"e35337e7a2844a5e8b294e94a971ab2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c9b2480264146d09b74151499d89e2b","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9ec14e36ee6452781f5dd4b1379359c","value":125}},"7d2cbedff1504352894e9edfc903944a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a8841c959294fc4ad7cb730941e4a1a","placeholder":"​","style":"IPY_MODEL_0e85ac433f574dec98f6394360e37ed9","value":" 125/125 [00:00&lt;00:00, 11.6kB/s]"}},"c78a646392944f8cbe182e5f8d84728f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0800568090a4a23b3864b5f25f255fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee5ff464ee224a7692092448dc2dc7dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c9b2480264146d09b74151499d89e2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9ec14e36ee6452781f5dd4b1379359c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a8841c959294fc4ad7cb730941e4a1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e85ac433f574dec98f6394360e37ed9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"060bf7d9a8d64680ab82e5a0ab6a43a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c49cd1a909f44512965a5a7227c14a5a","IPY_MODEL_0a068df99c3b464aa0c3e13d2102ad11","IPY_MODEL_3e47130f7327408d83fe058ed826616f"],"layout":"IPY_MODEL_a84a7d5874574c49affde5734b14c177"}},"c49cd1a909f44512965a5a7227c14a5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_746eb77342e245ab868ddf5e9d26db95","placeholder":"​","style":"IPY_MODEL_fd75f77153fc49d5afbbffa83cd32e1e","value":"tokenizer.json: 100%"}},"0a068df99c3b464aa0c3e13d2102ad11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9013d7a58e3246e494618a5a9f96b2b9","max":13631023,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1a228be5ed7420ebe681ddec6859fc1","value":13631023}},"3e47130f7327408d83fe058ed826616f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c417fecdd7ad4daa8f430552ac62bfe5","placeholder":"​","style":"IPY_MODEL_b675fa5720da44e8907635f0d2015bf7","value":" 13.6M/13.6M [00:00&lt;00:00, 303MB/s]"}},"a84a7d5874574c49affde5734b14c177":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"746eb77342e245ab868ddf5e9d26db95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd75f77153fc49d5afbbffa83cd32e1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9013d7a58e3246e494618a5a9f96b2b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1a228be5ed7420ebe681ddec6859fc1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c417fecdd7ad4daa8f430552ac62bfe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b675fa5720da44e8907635f0d2015bf7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5ac6a16fa0b4d3aa00465ba1cb7d3c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_375ab4510f74413f9c166c4e720c7b2f","IPY_MODEL_be70c170dddd4693a7eae36d4f29578f","IPY_MODEL_dfdabb3bc97540c0aaaad9975ece0c1e"],"layout":"IPY_MODEL_95b39a516e5c41b794a6eeae4b3d599a"}},"375ab4510f74413f9c166c4e720c7b2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc3b44b855b14fab946be24639691007","placeholder":"​","style":"IPY_MODEL_f3d2f6e4a5e54d7d8cb682fb96749610","value":"tokenizer_config.json: 100%"}},"be70c170dddd4693a7eae36d4f29578f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d29ef23e0d884beb9d71dc96681a0b3e","max":367,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6abc0aa570754e93a660c57263313973","value":367}},"dfdabb3bc97540c0aaaad9975ece0c1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4317f7c8c5244206b092af3816e56b31","placeholder":"​","style":"IPY_MODEL_60f4ea2abead4ed8a6c7b11bb7ea8d63","value":" 367/367 [00:00&lt;00:00, 31.1kB/s]"}},"95b39a516e5c41b794a6eeae4b3d599a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc3b44b855b14fab946be24639691007":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3d2f6e4a5e54d7d8cb682fb96749610":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d29ef23e0d884beb9d71dc96681a0b3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6abc0aa570754e93a660c57263313973":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4317f7c8c5244206b092af3816e56b31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60f4ea2abead4ed8a6c7b11bb7ea8d63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d49912fb0bc84f95ae960d093661c252":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aaacf8b7a3394e159f8a2a3efa5d8962","IPY_MODEL_e141ae34949d4a0da53588867fa66f87","IPY_MODEL_3eb203527d624bb29cfd6c685848fa48"],"layout":"IPY_MODEL_75daacccf44a460a8a5a533970e69ab2"}},"aaacf8b7a3394e159f8a2a3efa5d8962":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2bcf199551049018223c11dd4b486d4","placeholder":"​","style":"IPY_MODEL_ae2c9643bb864fa48969d03f546cb91a","value":"vocab.txt: 100%"}},"e141ae34949d4a0da53588867fa66f87":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52c79f7754cf4b50b2c89ed2513b6278","max":5220781,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68f31a8016754c21a661d30fdfd6db71","value":5220781}},"3eb203527d624bb29cfd6c685848fa48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98fb70200a94471e8db9b2448a36e79b","placeholder":"​","style":"IPY_MODEL_398dde2e33114449b432ce96bb634369","value":" 5.22M/5.22M [00:00&lt;00:00, 33.2MB/s]"}},"75daacccf44a460a8a5a533970e69ab2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2bcf199551049018223c11dd4b486d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae2c9643bb864fa48969d03f546cb91a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52c79f7754cf4b50b2c89ed2513b6278":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68f31a8016754c21a661d30fdfd6db71":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98fb70200a94471e8db9b2448a36e79b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"398dde2e33114449b432ce96bb634369":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Scenario 3 : Lama2-13b-chat-german model\n","\n","\n","*   With Conversation chain\n","*   With English Prompt\n","*   Conversation Summary memory - keeping summary of last messages in memory\n","*   Machine Manual Truncated (page 6-41 based on discussions)\n","\n","**Prompt (Default llama2)**:\n","\n","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n","\n","If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n","\n","\n","\n","### Usage V100 Doesn't Crash:\n","With 16 GB of GPU :\n","\n","*   10-11 GB for model and embedder\n","*   5 GB of RAM (in use)\n","\n","***Colab Cost: Approx 5.45 units/hour***\n"],"metadata":{"id":"USwmqbq0UIVP"}},{"cell_type":"code","source":["experiment_name = \"scenario_3\" # going to be used as a file name to store results"],"metadata":{"id":"WjQKe95uyrw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzbdMaFiKhGO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701079541120,"user_tz":-60,"elapsed":44943,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"30793694-7411-46ae-8f71-8b8b3749ddff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken\n","  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting chromadb\n","  Downloading chromadb-0.4.18-py3-none-any.whl (502 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting InstructorEmbedding\n","  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n","Collecting accelerate\n","  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes\n","  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Xformers\n","  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pymupdf\n","  Downloading PyMuPDF-1.23.6-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.0)\n","Collecting aleph_alpha_client\n","  Downloading aleph_alpha_client-4.1.0-py3-none-any.whl (35 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n","Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting langsmith<0.1.0,>=0.0.63 (from langchain)\n","  Downloading langsmith-0.0.66-py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n","Collecting chroma-hnswlib==0.7.3 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n","  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n","  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n","Collecting pulsar-client>=3.1.0 (from chromadb)\n","  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.2)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-4.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (72 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n","Collecting PyMuPDFb==1.23.6 (from pymupdf)\n","  Downloading PyMuPDFb-1.23.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from aleph_alpha_client) (2.0.7)\n","Collecting aiodns>=3.0.0 (from aleph_alpha_client)\n","  Downloading aiodns-3.1.1-py3-none-any.whl (5.4 kB)\n","Collecting aiohttp-retry>=2.8.3 (from aleph_alpha_client)\n","  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from aleph_alpha_client) (9.4.0)\n","Collecting python-liquid>=1.9.4 (from aleph_alpha_client)\n","  Downloading python_liquid-1.10.0-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pycares>=4.0.0 (from aiodns>=3.0.0->aleph_alpha_client)\n","  Downloading pycares-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions>=4.5.0 (from chromadb)\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Collecting urllib3>=1.26 (from aleph_alpha_client)\n","  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n","Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n","Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n","Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n","Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n","Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n","Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from pycares>=4.0.0->aiodns>=3.0.0->aleph_alpha_client) (1.16.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns>=3.0.0->aleph_alpha_client) (2.21)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n","Building wheels for collected packages: sentence-transformers, pypika\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=96918e6a4d2c2402a69b0902afebb112320c742fdd0fee778f631533457615f1\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=e739b2371024171322bc46dba86246b82082c59f18b236da0821f10df8f4bce9\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built sentence-transformers pypika\n","Installing collected packages: sentencepiece, pypika, monotonic, mmh3, InstructorEmbedding, bitsandbytes, websockets, uvloop, urllib3, typing-extensions, python-dotenv, PyMuPDFb, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, humanfriendly, httptools, h11, einops, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typing-inspect, starlette, python-liquid, pymupdf, pycares, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, coloredlogs, asgiref, Xformers, tiktoken, posthog, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langsmith, fastapi, dataclasses-json, aiohttp-retry, aiodns, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain, kubernetes, accelerate, opentelemetry-instrumentation-fastapi, aleph_alpha_client, sentence-transformers, chromadb\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.7\n","    Uninstalling urllib3-2.0.7:\n","      Successfully uninstalled urllib3-2.0.7\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed InstructorEmbedding-1.0.1 PyMuPDFb-1.23.6 Xformers-0.0.22.post7 accelerate-0.24.1 aiodns-3.1.1 aiohttp-retry-2.8.3 aleph_alpha_client-4.1.0 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.0.1 bitsandbytes-0.41.2.post2 chroma-hnswlib-0.7.3 chromadb-0.4.18 coloredlogs-15.0.1 dataclasses-json-0.6.2 deprecated-1.2.14 einops-0.7.0 fastapi-0.104.1 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 jsonpatch-1.33 jsonpointer-2.4 kubernetes-28.1.0 langchain-0.0.340 langsmith-0.0.66 marshmallow-3.20.1 mmh3-4.0.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.16.3 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pycares-4.4.0 pymupdf-1.23.6 pypika-0.48.9 python-dotenv-1.0.0 python-liquid-1.10.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 starlette-0.27.0 tiktoken-0.5.1 typing-extensions-4.8.0 typing-inspect-0.9.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"]}],"source":["!pip install langchain tiktoken chromadb einops transformers InstructorEmbedding accelerate bitsandbytes sentencepiece Xformers sentence-transformers pymupdf safetensors aleph_alpha_client"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LqUlJT1KsKKz","executionInfo":{"status":"ok","timestamp":1701079566073,"user_tz":-60,"elapsed":24969,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"c1e390a4-ea40-4ba0-a6f4-b6c1f42bb648"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import transformers\n","from torch import cuda, bfloat16\n","\n","\n","device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n","\n","# set quantization configuration to load large model with less GPU memory\n","# this requires the `bitsandbytes` library\n","bnb_config = transformers.BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_use_double_quant=True, # using double quant\n","    bnb_4bit_compute_dtype=bfloat16\n",")\n","\n","\n","model_name = \"/content/drive/MyDrive/Colab Notebooks/LLMS/Llama-2-13b-chat-german\"\n","\n","model = transformers.AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True,\n","    quantization_config=bnb_config,\n","    device_map=device\n","    )\n","model.eval()"],"metadata":{"id":"-IJs5Tvr89Rj","colab":{"base_uri":"https://localhost:8080/","height":590,"referenced_widgets":["ee2403a5567c48be8c47c9f5073b9778","baa9fcf56dfb4559b75508b2ebc13e42","5980ad47c8b34e4a98872695d8a35ba5","8fbe6c606f7b4dee9c594e76ca12541b","71689dcee48649b1af58b0dc830275b3","41201c718c8b4775a1b8abb590e9ce4c","deffb3d2946642e19f02b146ea709aec","950d26323d2e41a7aac9c700fa5d6821","8d9378757bb140fb9feaed29055b711c","a4c68f68b8e14894ba4b142c097d9b43","64b1d3031ff840dbbadcb6d04c12bb7a"]},"executionInfo":{"status":"ok","timestamp":1701079857302,"user_tz":-60,"elapsed":291240,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"0c0f9385-eb2e-46f4-d800-e136d386035f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2403a5567c48be8c47c9f5073b9778"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n","    (layers): ModuleList(\n","      (0-39): 40 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",")"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"0tYDPwWMv1f4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.eos_token_id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZgHk_fpgdl3","executionInfo":{"status":"ok","timestamp":1701079859003,"user_tz":-60,"elapsed":21,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"331eabd5-7e33-4195-a88c-13749c5a14cb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from transformers import pipeline, TextStreamer\n","from langchain.llms import HuggingFacePipeline\n","import torch\n","streamer = TextStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    # max_length=2048, should be replaced by max_new_tokens\n","    # temperature=1.0,\n","    max_new_tokens = 1024,\n","    top_p=0.95,\n","    top_k = 10,\n","    repetition_penalty=1.15,\n","    # streamer=streamer,\n","    eos_token_id=tokenizer.eos_token_id,\n","    # do_sample = True\n",")\n","\n","local_llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.0})"],"metadata":{"id":"p5WceJejLNgn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.vectorstores import Chroma\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","from langchain.document_loaders import PyMuPDFLoader"],"metadata":{"id":"kYxIiryhQzaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceEmbeddings\n","\n","embeddings = HuggingFaceEmbeddings(model_name= \"setu4993/LaBSE\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342,"referenced_widgets":["7ef43c9292364bb9b7897690815fdfd3","b3274cc318c048e88205ab9bbc7f750c","cf289ebc7dc44d9cbb112789f7d6c8ff","b93f75562017477eb33ac1f68e015405","64e9d009bf864ea294f2317e3a00470c","51f839df7d9449aaae63aa5914cfd521","45d1dcdb0d484192aee67ea7137922e1","894a074634d14291aa2920766b0b52fa","61d3c0e2760b4f6d9b13184245818857","0cc1c44f071c4ccdbbccd06f7e2d8086","2d25385b096e49ca906eadd8b93f8b6c","1fe43a063ea24e7e9fb10b44357fcf08","4235128547b14435af39d5371948df50","8b218290f81a404bb52308afe1af1ee5","3985b79764524e4fb8ee9d59f734ed5a","67a0b7101c584f928a1f069a841c83d5","877db5398c124ec1a9dc92f2c9a3c988","1e43616097504bcb81ad2001b77a71af","97923982ba5c46b0b46b19537c2fe91c","474c14ce597b4f8b984e42ef468e90c4","0664e306fea54a6fae7baa3beda86691","b568cfe2470c46c299796239a37bd6fd","3c0f13ff27114ea08e376746c4fda4fc","bf83e4d45beb4e7db3c7a284e541153b","b1e17b91c6f84c989476aaccc19d4bd0","7c00db19a9284cb2bd9cc231218f7bf9","894047e08a9345179ae0e0c33c909ba9","2b0b07fcb42845079557b61dd83db4d5","64790a4f3def47eb86242f5482a6f9b2","7748662039314055b53496ec90242ea9","c39e656d1c0446baa6f4092c5645da25","f72ac175581a424bb829c2137f6bd729","f74f535b35934fe281830a71174d3da0","29e5014988034fac829421f0a28e6bcd","36f8f48d83464a5983525dec6822564c","887e97210e344388ac00b6bcc8be2909","e3f94a37bbd14bbfbd955d826f6bb5c0","0785a08a1d9e415c807f55502cdd9eb0","83ea489e3ed648a888df0ef935d1ad83","727b68f80c8d47ad8b0221c90ab254dd","ca40565293434479900ae816f8edcf74","34b5aad4a1724c6593539410158d249b","6c929f26c81446ef8d1c2967f51d7e75","7a2071d9cd2c4796808c52a4a419d9a9","5ef6f570f03f4064bb9f4bfe96b0f2eb","059a274c14f54643b7b1f23098e23fef","12a78cc1f6ef4037b985c19a1ea61822","be261006453f4e76afb449948901e760","0cd5f27cecae497cad778342d789afe1","40b050d5fefa45359df7d2d7d74578d0","c9112162ab6f4ff8970fe4aa913d7256","68c1ce029d384d5a87803637ec89b880","c8d2db81c87d4d2391327aac0477e53e","ab502759045c48b18f92d72bf9c14925","058cec556a014c3b990f56cb7c13b77e","3fba5b84bbdb4c19bde36812e314bc8b","b7270b41e3c94f51b56537fd4ba54c30","e35337e7a2844a5e8b294e94a971ab2e","7d2cbedff1504352894e9edfc903944a","c78a646392944f8cbe182e5f8d84728f","a0800568090a4a23b3864b5f25f255fc","ee5ff464ee224a7692092448dc2dc7dd","1c9b2480264146d09b74151499d89e2b","a9ec14e36ee6452781f5dd4b1379359c","8a8841c959294fc4ad7cb730941e4a1a","0e85ac433f574dec98f6394360e37ed9","060bf7d9a8d64680ab82e5a0ab6a43a0","c49cd1a909f44512965a5a7227c14a5a","0a068df99c3b464aa0c3e13d2102ad11","3e47130f7327408d83fe058ed826616f","a84a7d5874574c49affde5734b14c177","746eb77342e245ab868ddf5e9d26db95","fd75f77153fc49d5afbbffa83cd32e1e","9013d7a58e3246e494618a5a9f96b2b9","c1a228be5ed7420ebe681ddec6859fc1","c417fecdd7ad4daa8f430552ac62bfe5","b675fa5720da44e8907635f0d2015bf7","c5ac6a16fa0b4d3aa00465ba1cb7d3c8","375ab4510f74413f9c166c4e720c7b2f","be70c170dddd4693a7eae36d4f29578f","dfdabb3bc97540c0aaaad9975ece0c1e","95b39a516e5c41b794a6eeae4b3d599a","cc3b44b855b14fab946be24639691007","f3d2f6e4a5e54d7d8cb682fb96749610","d29ef23e0d884beb9d71dc96681a0b3e","6abc0aa570754e93a660c57263313973","4317f7c8c5244206b092af3816e56b31","60f4ea2abead4ed8a6c7b11bb7ea8d63","d49912fb0bc84f95ae960d093661c252","aaacf8b7a3394e159f8a2a3efa5d8962","e141ae34949d4a0da53588867fa66f87","3eb203527d624bb29cfd6c685848fa48","75daacccf44a460a8a5a533970e69ab2","a2bcf199551049018223c11dd4b486d4","ae2c9643bb864fa48969d03f546cb91a","52c79f7754cf4b50b2c89ed2513b6278","68f31a8016754c21a661d30fdfd6db71","98fb70200a94471e8db9b2448a36e79b","398dde2e33114449b432ce96bb634369"]},"id":"0bGBf4TChUBq","executionInfo":{"status":"ok","timestamp":1701079884043,"user_tz":-60,"elapsed":20335,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"1d0c36ea-7742-435b-8191-4ade6e41d2ae"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":[".gitattributes:   0%|          | 0.00/496 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef43c9292364bb9b7897690815fdfd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/4.41k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe43a063ea24e7e9fb10b44357fcf08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c0f13ff27114ea08e376746c4fda4fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29e5014988034fac829421f0a28e6bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef6f570f03f4064bb9f4bfe96b0f2eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fba5b84bbdb4c19bde36812e314bc8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060bf7d9a8d64680ab82e5a0ab6a43a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5ac6a16fa0b4d3aa00465ba1cb7d3c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d49912fb0bc84f95ae960d093661c252"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/setu4993_LaBSE. Creating a new one with MEAN pooling.\n"]}]},{"cell_type":"markdown","source":["# Loading data and Splitting"],"metadata":{"id":"jxUrrRCHu6Fk"}},{"cell_type":"code","source":["loader = PyMuPDFLoader('/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf')"],"metadata":{"id":"iMqqorzcR7ci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["documents = loader.load_and_split() # single document"],"metadata":{"id":"uHuVOOPCQz02"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["documents[1].page_content"],"metadata":{"id":"XNdPgblVFyac","colab":{"base_uri":"https://localhost:8080/","height":192},"executionInfo":{"status":"ok","timestamp":1700730769360,"user_tz":-60,"elapsed":343,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"303bf956-fe8c-434b-8862-080a1d23c25c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'7\\nDE\\n•�Versuchen�Sie�niemals,�die�Waschmaschinentür�mit�Gewalt�zu�öffnen.�Die�Wasch-\\nmaschinentür lässt sich erst ein paar Minuten nach dem Ende des Waschpro-\\ngramms öffnen. Falls Sie versuchen, die Tür gewaltsam zu öffnen, können Tür und \\nVerriegelungsmechanismus beschädigt werden.\\n•�Ziehen�Sie�den�Netzstecker,�wenn�Sie�das�Gerät�nicht�benutzen.\\n•�Gießen�Sie�niemals�Wasser�oder�andere�Flüssigkeiten�direkt�auf�das�Gerät!�Es�\\nbesteht Stromschlaggefahr!\\n•�Berühren�Sie�den�Stecker�niemals�mit�feuchten�oder�gar�nassen�Händen!�Ziehen�\\nSie den Netzstecker niemals am Kabel aus der Steckdose: Fassen Sie grundsätz-\\nlich den Stecker selbst.\\n•�Verwenden� Sie� ausschließlich� für� Waschmaschinen� geeignete� Waschmittel,�\\nWeichspüler und Zusatzstoffe.\\n•�Halten�Sie�sich�an�die�Hinweise�auf�Pflegeetiketten�und�auf�der�Waschmittelpa-\\nckung.\\n•�Vor�Aufstellung,�Wartung,�Reinigung�und�vor�Reparaturen�muss�unbedingt�der�\\nNetzstecker gezogen werden.\\n•�Lassen�Sie�Installations-�und�Reparaturarbeiten�grundsätzlich�vom�autorisier-\\nten Kundendienst ausführen. Der Hersteller haftet nicht bei Schäden, die \\ndurch den Eingriff nicht autorisierter Personen entstehen.\\n•�Falls�das�Netzkabel�beschädigt�wird,�muss�es�vom�Hersteller,�seinem�Kunden-\\ndienst, einer vom Importeur bestimmten Stelle oder einer gleichermaßen qualifi-\\nzierten Person (am besten einem Elektriker) ausgetauscht werden. \\n•�Stellen�Sie�das�Produkt�auf�einen�festen,�flachen�und�ebenen�Untergrund.\\n•�Stellen�Sie�das�Gerät�nicht�auf�langflorigen�Teppichen�oder�ähnlichen�Unterlagen�\\nauf.\\n•�Stellen�Sie�das�Produkt�nicht�auf�eine�hohe�Plattform�oder�in�die�Nähe�der�Kante�\\neines kaskadierten Untergrundes.\\n•�Stellen�Sie�das�Gerät�nicht�auf�das�Netzkabel.\\n•�Benutzen�Sie�niemals�Scheuerschwämme�oder�andere�Scheuermittel.�Solche�Mit-\\nte beschädigen lackierte und verchromte Flächen sowie Kunststoffteile.\\n1.2 Bestimmungsgemäßer Einsatz\\n•�Dieses�Gerät�wurde�für�den�Hausgebrauch�entwickelt.�Es�darf�nicht�für�kommer-\\nzielle Zwecke und nicht außerhalb seines bestimmungsgemäßen Einsatzgebietes \\neingesetzt werden.\\n•�Das�Gerät�darf�nur�zum�Waschen�und�Spülen�von�Textilien�verwendet�werden,�die�\\nentsprechend gekennzeichnet sind.\\n•�Der�Hersteller�haftet�nicht�bei�Schäden,�die�durch�falschen�Gebrauch�oder�un-\\nsachgemäßen Transport entstehen.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["#splitting the text into\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","texts = text_splitter.split_documents(documents)"],"metadata":{"id":"SW1hiEVRR173"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(texts)"],"metadata":{"id":"kjFfrqp-Hzo4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700730778032,"user_tz":-60,"elapsed":7,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"c3ee8613-2939-455d-c597-e8d42dfa215b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["213"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["\n","# from langchain.embeddings import HuggingFaceInstructEmbeddings\n","\n","# instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n","#                                                       model_kwargs={\"device\": \"cuda\"})\n"],"metadata":{"id":"Hbnp8UgeSTI1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Embed and store the texts\n","# Supplying a persist_directory will store the embeddings on disk\n","persist_directory = \"/content/drive/MyDrive/Colab Notebooks/manual\"\n","# persist_directory = \"db\"\n","\n","## Here is the nmew embeddings being used\n","# embedding = instructor_embeddings\n","\n","vectordb = Chroma.from_documents(documents=texts, collection_name=\"manual-truncated-labse\",\n","                                 embedding=embeddings,\n","                                 persist_directory=persist_directory)\n","\n","# must be run when using in notebook otherwise db will not persist automatically\n","vectordb.persist()"],"metadata":{"id":"XYSO_xfzSYML"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load already stored documents"],"metadata":{"id":"hHG0Yd58DR_o"}},{"cell_type":"markdown","source":["# Load alreday stored data from drive"],"metadata":{"id":"mO7Nc9wgvQQU"}},{"cell_type":"code","source":["persist_directory = \"/content/drive/MyDrive/Colab Notebooks/manual\""],"metadata":{"id":"tm_Ml0XtSW8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectordb = Chroma(embedding_function=embeddings, collection_name=\"manual-truncated-labse\",\n","                                 persist_directory=persist_directory)"],"metadata":{"id":"mUqNr2zsZNkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectordb.similarity_search(\"what is the document about?\")"],"metadata":{"id":"13dy5kIfFTfY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701079890617,"user_tz":-60,"elapsed":5239,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"1f7e8d5b-ba49-461c-c8e7-635b7ba549b8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='der Schleudergeschwindigkeit sowie Schwankungen der Versorgungsspannung ändern.\\nC \\nDie Dauer des ausgewählten Programms erscheint im Display der Maschine. Dabei kann es \\ndurchaus zu kleinen Abweichungen zwischen der angezeigten und der tatsächlichen Dauer kommen.\\nRichtwerte für Synthetik-Programme (DE)\\n Beladung (kg) \\nWasserverbrauch \\n(l)\\nEnergieverbrauch \\n(kWh)\\nProgrammdauer \\n(Min)*\\nRestfeuchtigkeit (%) **\\nRestfeuchtigkeit (%) **\\n≤ 1000 rpm\\n> 1000 rpm\\nPflegeleicht 60\\n3\\n45\\n1.00\\n01:48\\n45\\n40', metadata={'author': '', 'creationDate': '', 'creator': '', 'file_path': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231123091019Z', 'page': 19, 'producer': 'iLovePDF', 'source': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}),\n"," Document(page_content='Waschen richtig behandelt werden. Fragen Sie \\nim Zweifelsfall bei einer chemischen Reinigung \\nnach.\\n•�Verwenden�Sie�ausschließlich�Färbemittel�oder�\\nMittel zur Kalkentfernung, die sich ausdrücklich \\nzur Verwendung in der Waschmaschine eignen. \\nBeachten Sie dabei immer die Hinweise auf der \\nVerpackung.\\n•�Waschen�Sie�Hosen�und�empfindliche�\\nKleidungsstücke „auf links“, also mit der \\nInnenseite nach außen.\\n•�Legen�Sie�Wäschestücke�aus�Angorawolle�\\nvor dem Waschen ein paar Stunden in das', metadata={'author': '', 'creationDate': '', 'creator': '', 'file_path': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231123091019Z', 'page': 9, 'producer': 'iLovePDF', 'source': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}),\n"," Document(page_content='> 1000 rpm\\nPflegeleicht 60\\n3\\n45\\n1.00\\n01:48\\n45\\n40\\nPflegeleicht 40\\n3\\n45\\n0.45\\n01:23\\n45\\n40\\n* Die Dauer des ausgewählten Programms erscheint im Display der Maschine. Dabei kann es durchaus zu kleinen \\nAbweichungen zwischen der angezeigten und der tatsächlichen Dauer kommen.\\n** Die Restfeuchtigkeit hängt von der gewählten Schleudergeschwindigkeit ab.', metadata={'author': '', 'creationDate': '', 'creator': '', 'file_path': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231123091019Z', 'page': 19, 'producer': 'iLovePDF', 'source': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}),\n"," Document(page_content='•�Lassen�Sie�die�Waschmittelschublade�niemals�\\ngeöffnet, während ein Waschprogramm läuft!', metadata={'author': '', 'creationDate': '', 'creator': '', 'file_path': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231123091019Z', 'page': 10, 'producer': 'iLovePDF', 'source': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''})]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import textwrap\n","from collections import defaultdict\n","\n","def wrap_text_preserve_newlines(text, width=110):\n","    # Split the input text into lines based on newline characters\n","    lines = text.split('\\n')\n","\n","    # Wrap each line individually\n","    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n","\n","    # Join the wrapped lines back together using newline characters\n","    wrapped_text = '\\n'.join(wrapped_lines)\n","\n","    return wrapped_text\n","\n","def process_llm_response(llm_response):\n","    answer = wrap_text_preserve_newlines(llm_response['answer'])\n","    print(answer)\n","    print('\\n\\nSources:')\n","    sources = defaultdict(list)\n","    for source in llm_response[\"source_documents\"]:\n","        source_document = source.metadata['source']\n","        source_page = source.metadata['page']\n","        sources[source_document].append(source_page)\n","        print(source_document, source_page)\n","    return answer, sources"],"metadata":{"id":"wiL6-LOhXGt0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Langchain ConversationalRetrievalChain"],"metadata":{"id":"lKUXzGJgwDpn"}},{"cell_type":"code","source":["from langchain.chains import ConversationalRetrievalChain\n","from langchain.memory import ConversationSummaryMemory"],"metadata":{"id":"ScoM6HAxgCMq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# With llama2 default prompt.\n","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n","\n","If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n"],"metadata":{"id":"1sflOWt2h7QE"}},{"cell_type":"code","source":["## Default LLaMA-2 prompt style\n","B_INST, E_INST = \"[INST]\", \"[/INST]\"\n","B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n","context = \"CONTEXT:/n/n {context}/n\"\n","DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n","\n","If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n","\n","\"\"\"\n","\n","def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n","    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n","    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n","    return prompt_template\n","\n","# def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n","#     SYSTEM_PROMPT = B_INST + B_SYS + new_system_prompt + context + E_SYS\n","#     question_prompt =  instruction + E_INST\n","#     return SYSTEM_PROMPT, question_prompt"],"metadata":{"id":"hc907DAsiL1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n","\n","If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n","\n","instruction = \"\"\"CONTEXT:/n/n {context}/n\n","\n","Question: {question}\"\"\"\n","prompt_template = get_prompt(instruction, sys_prompt)"],"metadata":{"id":"4B3K0HDeyxDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts import (\n","    ChatPromptTemplate,\n","    HumanMessagePromptTemplate,\n","    SystemMessagePromptTemplate,\n",")\n","instructions = \"\"\"\n","\n","Question: {question}\"\"\"\n","SYSTEM_PROMPT, question_prompt = get_prompt(instructions)\n","\n","system_message_prompt = SystemMessagePromptTemplate.from_template(\n","    SYSTEM_PROMPT\n",")\n","human_message_prompt = HumanMessagePromptTemplate.from_template(\n","    question_prompt\n",")"],"metadata":{"id":"cY7NpHFqi1b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts.prompt import PromptTemplate\n","\n","A_PROMPT = PromptTemplate(\n","    template=prompt_template, input_variables=[\"context\", \"question\"]\n",")\n"],"metadata":{"id":"P8M3SHMTynNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A_PROMPT"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vELo8CSUzVL6","executionInfo":{"status":"ok","timestamp":1701079911650,"user_tz":-60,"elapsed":404,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"3de2443c-4f2d-464d-804a-3f2ef0b48ed3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PromptTemplate(input_variables=['context', 'question'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\nCONTEXT:/n/n {context}/n\\n\\nQuestion: {question}[/INST]\")"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# conversation summary memory. a seperate summry prompt can also be provided here\n","memory = ConversationSummaryMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer', llm = local_llm)"],"metadata":{"id":"DUF6E_xjNRGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pdf_qa = ConversationalRetrievalChain.from_llm(local_llm , vectordb.as_retriever(),\n","                                               memory=memory, return_source_documents=True, combine_docs_chain_kwargs={\"prompt\": A_PROMPT})"],"metadata":{"id":"bpTaH7nni15h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectordb.similarity_search(\"what is the document about?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZz-kl75apPX","executionInfo":{"status":"ok","timestamp":1701082895707,"user_tz":-60,"elapsed":9,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"27dd23ec-7275-4424-9ea0-6af364c7afb1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='der Schleudergeschwindigkeit sowie Schwankungen der Versorgungsspannung ändern.\\nC \\nDie Dauer des ausgewählten Programms erscheint im Display der Maschine. Dabei kann es \\ndurchaus zu kleinen Abweichungen zwischen der angezeigten und der tatsächlichen Dauer kommen.\\nRichtwerte für Synthetik-Programme (DE)\\n Beladung (kg) \\nWasserverbrauch \\n(l)\\nEnergieverbrauch \\n(kWh)\\nProgrammdauer \\n(Min)*\\nRestfeuchtigkeit (%) **\\nRestfeuchtigkeit (%) **\\n≤ 1000 rpm\\n> 1000 rpm\\nPflegeleicht 60\\n3\\n45\\n1.00\\n01:48\\n45\\n40', metadata={'author': '', 'creationDate': '', 'creator': '', 'file_path': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231123091019Z', 'page': 19, 'producer': 'iLovePDF', 'source': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}),\n"," Document(page_content='Waschen richtig behandelt werden. Fragen Sie \\nim Zweifelsfall bei einer chemischen Reinigung \\nnach.\\n•�Verwenden�Sie�ausschließlich�Färbemittel�oder�\\nMittel zur Kalkentfernung, die sich ausdrücklich \\nzur Verwendung in der Waschmaschine eignen. \\nBeachten Sie dabei immer die Hinweise auf der \\nVerpackung.\\n•�Waschen�Sie�Hosen�und�empfindliche�\\nKleidungsstücke „auf links“, also mit der \\nInnenseite nach außen.\\n•�Legen�Sie�Wäschestücke�aus�Angorawolle�\\nvor dem Waschen ein paar Stunden in das', metadata={'author': '', 'creationDate': '', 'creator': '', 'file_path': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231123091019Z', 'page': 9, 'producer': 'iLovePDF', 'source': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}),\n"," Document(page_content='> 1000 rpm\\nPflegeleicht 60\\n3\\n45\\n1.00\\n01:48\\n45\\n40\\nPflegeleicht 40\\n3\\n45\\n0.45\\n01:23\\n45\\n40\\n* Die Dauer des ausgewählten Programms erscheint im Display der Maschine. Dabei kann es durchaus zu kleinen \\nAbweichungen zwischen der angezeigten und der tatsächlichen Dauer kommen.\\n** Die Restfeuchtigkeit hängt von der gewählten Schleudergeschwindigkeit ab.', metadata={'author': '', 'creationDate': '', 'creator': '', 'file_path': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231123091019Z', 'page': 19, 'producer': 'iLovePDF', 'source': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}),\n"," Document(page_content='•�Lassen�Sie�die�Waschmittelschublade�niemals�\\ngeöffnet, während ein Waschprogramm läuft!', metadata={'author': '', 'creationDate': '', 'creator': '', 'file_path': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231123091019Z', 'page': 10, 'producer': 'iLovePDF', 'source': '/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''})]"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["import gc\n","import torch\n","# to flush the pytorch memory manually\n","def flush():\n","  gc.collect()\n","  torch.cuda.empty_cache()\n","  torch.cuda.reset_peak_memory_stats()"],"metadata":{"id":"aCYBzwFIazFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# iterating through all queries and storing the results in csv/excel for evaluation\n","questions_list = [\"what is the document about?\",\n","                  \"Was muss ich beim Filter beachten?\",\n","                  \"Welche Schäden können an der Waschmaschine im Betrieb entstehen?\",\n","                  \"Wo finde ich Sammelstellen für Altgeräte?\",\n","                  \"Welche Gefahren habe ich mit dieser Waschmaschine?\",\n","                  \"Wo kann ich Waschmittel in die Waschmaschine einfüllen?\",\n","                  \"Was muss ich bei der Aufstellung der Waschmaschine beachten?\",\n","                  \"Kann man das Wasser des Schlauchs trinken?\",\n","                  \"Kann ich die Maschinentür öffnen, wenn Wasser in der Maschine sichtbar ist?\",\n","                  \"Wie kann ich den Weichspüler in der Maschine verwenden?\",\n","                  \"Welche Temperatur wird für bunte und stark verschmutzte Wäsche empfohlen?\",\n","                  \"Wie lauten die Anweisungen für helle Farben und extrem verschmutzte Wäsche?\",\n","                  \"Wie bereite ich die Wäsche zum Waschen vor?\",\n","                  \"Wie kann man Wäsche sortieren?\",\n","                  \"Wie lauten die Anweisungen für den Anschluss des Wasserablaufs?\",\n","                  \"Welche Gefahren habe ich mit dieser Waschmaschine?\",\n","                  \"Kann ich mit dem Wasser aus der Waschmaschine meine Katze waschen?\"\n","                  ]\n","results = defaultdict(list)\n","for query in questions_list:\n","  response = pdf_qa({\"question\": query})\n","  answer, sources = process_llm_response(response)\n","  results[\"question\"].append(query)\n","  results[\"answer\"].append(answer)\n","  results[\"memory\"].append(memory.buffer)\n","  results[\"sources\"].append(sources)\n","  flush()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ziGs6XXGup2N","outputId":"fd8c1c8b-490d-4869-ad85-93df3bf9da5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Basierend auf dem Inhalt des Dokuments scheint es für externe Veröffentlichung konzipiert zu sein. Es enthält\n","detaillierte Anweisungen zum richtigen Umgang mit dem Produkt, was darauf hinweist, dass es für Benutzer\n","entwickelt wurde, die das Gerät extern verwenden. Darüber hinaus ist der Dokumentenkopf \"Bedienungsanleitung\"\n","und es gibt keine speziellen internen Informationen, die nur Mitarbeitern zugänglich sein sollten. Daher kann\n","man schließen, dass dieses Dokument für externe Verteilung konzipiert wurde.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 7\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 1\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 9\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Beim Filter musst du folgende Punkte beachten:\n","\n","1. Regelmäßige Reinigung: Der Pumpenfilter sollte mindestens alle drei Monate gereinigt werden, um eine\n","ordnungsgemäße Funktion sicherzustellen.\n","2. Ablässe des Wassers: Vor der Reinigung muss das Wasser im Filter abgelassen werden. In bestimmten Fällen\n","wie Umzug oder Frostgefahr muss das Wasser vollständig abgelassen werden.\n","3. Entfernen von Fremdkörpern: Im Filter können Fremdkörper zurückbleiben und die Maschine beschädigen sowie\n","starke Betriebsgeräusche verursachen. Es ist wichtig, diese Fremdkörper regelmäßig zu entfernen.\n","4. Beachten Sie die Anweisungen auf der Verpackung: Achten Sie immer auf die Anweisungen auf der Verpackung\n","des Filters und den speziellen Empfehlungen für seine Reinigung.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 9\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 26\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 32\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Bei unregelmäßiger Wäscheverteilung (Unwucht) in der Trommel schleudert die Maschine nicht, damit es nicht zu\n","Beschädigungen der Maschine oder ihrer Umgebung kommt. In diesem Fall sollte man die Wäsche auflockern,\n","gleichmäßiger verteilen und erneut schleudern. Die Programmdauer wird nicht heruntergezählt. Dies kann an\n","einer ungleichmäßigen Verteilung der Wäsche in der Maschine liegen. Bei sehr ungleichmäßiger Wäscheverteilung\n","spricht eine spezielle Schutzschaltung an.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 10\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 34\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Um Sammelstellen für Altgeräte zu finden, konsultiere bitte deine lokale Stadtverwaltung oder deinen Händler.\n","Sie können dir Informationen darüber geben, wo du Altgeräte entsorgen kannst und wie du sie effizient und\n","umweltfreundlich verwerten kannst.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 3\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 3\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 7\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Bei der Verwendung einer bestimmten Waschmaschine gibt es folgende Gefahren:\n","\n","1. Schlechtere Leistung: Wenn die Waschmaschine nicht ordnungsgemäß gewartet und unterhalten wird, kann sie\n","ihre Leistung verlieren und möglicherweise nicht mehr effizient waschen.\n","2. Verschmutzung: Eine unreinigte Trommel kann zu einer schlechteren Reinigung führen und Verschmutzungen an\n","der Wäsche hinterlassen.\n","3. Unangemessener Einsatz: Wenn die Waschmaschine nicht für den angedachten Zweck verwendet wird, kann dies zu\n","Problemen wie Überhärtung oder Beschädigung der Textilien führen.\n","4. Fehlkonfiguration: Falsche Konfiguration der Waschmaschine kann zu unerwarteten Ergebnissen oder\n","Beschädigungen der Textilien führen.\n","5. Unzureichende Reinigung: Nicht regelmäßiges Reinigen der Trommel kann zu einer schlechteren Reinigung der\n","Wäsche führen und zu Verschmutzungen an der Wäsche.\n","6. Unangemessene Menge an Waschmittel: Zu viel oder zu wenig Waschmittel kann zu unerwünschtem Ergebnis oder\n","Beschädigungen der Textilien führen.\n","7. Unangemessene Temperatur: Verwendung einer unangemessenen Temperatur kann zu unerwünschtem Ergebnis oder\n","Beschädigungen der Textilien führen.\n","8. Unangemessene Geschwindigkeit: Verwendung einer unangemessenen Geschwindigkeit kann zu unerwünschtem\n","Ergebnis oder Beschädigungen der Textilien führen.\n","9. Unangemessene Zeit: Verwendung einer unangemessenen Zeit kann zu unerwünschtem Ergebnis oder Beschädigungen\n","der Textilien führen.\n","10. Unangemessene Funktionen: Verwendung von Funktionen, die nicht für die spezifische Art von Textilien\n","geeignet sind, kann zu unerwünschtem Ergebnis oder Beschädigungen der Textilien führen.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 10\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 10\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 21\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Das Waschmittel sollte in das Hauptwäschefach (Fach 2) oder direkt zur Wäsche in die Trommel eingefüllt\n","werden. Bei Tablettenformen des Waschmittels sollten Sie es entweder in das Hauptwäschefach (Fach 2) oder\n","direkt zur Wäsche in die Trommel geben. Tab-Waschmittel können Rückstände im Waschmittelfach hinterlassen,\n","daher ist es wichtig, sie direkt zur Wäsche im unteren Bereich der Trommel zu geben.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 32\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 12\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 11\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 32\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Bei der Aufstellung der Waschmaschine solltest du folgende Punkte berücksichtigen:\n","\n","1. Platzierung: Stelle sicher, dass die Waschmaschine genügend Platz hat, um sich während des Betriebs\n","ausreichend zu bewegen. Es sollte auch eine ausreichende Entfernung zwischen der Maschine und den Wänden sein,\n","um unnötige Berührungen zu vermeiden.\n","2. Elektrik: Überprüfe, ob alle elektrischen Leitungen ordnungsgemäß verbunden sind und ob sie nicht\n","beschädigt oder überlastet sind. Stelle sicher, dass die Steckdosen und Schalter sicher montiert sind und\n","keine Risse oder Beschädigungen haben.\n","3. Wasseranschluss: Überprüfe den Wasseranschluss, um sicherzustellen, dass er ordnungsgemäß angeschlossen ist\n","und kein Leck ist. Stelle sicher, dass die Abdeckung des Anschlusses fest sitzt und keine Risse oder\n","Beschädigungen hat.\n","4. Sicherheitsmerkmale: Überprüfe die Sicherheitsmerkmale wie zum Beispiel die Sicherheitslücken, die\n","sicherstellen sollen, dass Kinder nicht hineinfallen können. Stelle sicher, dass diese Merkmale intakt sind\n","und keine Beschädigungen haben.\n","5. Benutzeroberfläche: Stelle sicher, dass die Bedienoberfläche klar und leicht lesbar ist. Überprüfe, ob alle\n","Knöpfe und Schalter ordnungsgemäß funktionieren und keine Beschädigungen haben.\n","6. Reinigung: Reinige die Waschmaschine vor dem ersten Einsatz gründlich, um Abfälle und Chemikalienreste zu\n","entfernen.\n","7. Regelmäßige Wartung: Befolge die von der Hersteller empfohlenen Wartungsintervalle für die Waschmaschine,\n","um ihre Leistung und Lebensdauer zu erhalten.\n","8. Umgebungskontrolle: Stelle sicher, dass die Waschmaschine in einem trockenen und wellenlosen Raum platziert\n","ist, um Unfälle zu verhindern.\n","9. Energieeffizienz: Wähle eine energieeffiziente Waschmaschine aus, um Treibhausgasemissionen und\n","Energiekosten zu reduzieren.\n","10. Einhaltung der Vorschriften: Stelle sicher, dass die Waschmaschine den geltenden Vorschriften entspricht\n","und keine potenziellen Risiken für die Gesundheit oder Sicherheit darstellt.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 32\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 10\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 9\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Nein, es ist nicht empfehlenswert oder sicher, das Wasser des Schlauchs zu trinken. Der Schlauch dient dazu,\n","Wasser von einer Quelle wie einem Waschbecken oder einer Dusche zur Maschine zu leiten. Das Wasser im Schlauch\n","könnte ungesund sein oder unangenehm schmeckend sein, da es möglicherweise Chemikalien enthält, die für\n","Reinigung und Pflege verwendet werden, oder andere Stoffe enthalten hat, die für den Gebrauch in der Maschine\n","bestimmt sind. Darüber hinaus könnten Schadstoffe oder Bakterien im Wasser vorhanden sein, die für Menschen\n","gesundheitsschädlich sein können. Es ist besser, sauberes Trinkwasser zu verwenden, um sicherzustellen, dass\n","du deine Maschine ordnungsgemäß wartest und sie effizient benutzt.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 5\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 5\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 6\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Nein, du solltest die Maschinentür nicht öffnen, wenn Wasser in der Maschine sichtbar ist. Dieses Verhalten\n","ist normal und bedeutet, dass die Maschine noch nicht ausreichend getrocknet hat. Es ist wichtig, auf diese\n","Anweisung zu achten, um den Verschleiß von Elektroenergie und die Lebensdauer der Komponenten der Maschine zu\n","reduzieren. Stattdessen solltest du die Maschine warten lassen, bis sie vollständig getrocknet ist, bevor du\n","versuchst, die Maschinentür zu öffnen.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 34\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 23\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 22\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Der Weichspüler sollte vor dem Einlegen der Wäsche in die Maschine verwendet werden. Er hilft dabei, die\n","Wäsche weicher zu machen und verringert das Risiko von Schäden an der Wäsche. Es ist wichtig, den Weichspüler\n","gemäß den Anweisungen des Herstellers zu verwenden.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 10\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 34\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 16\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Für bunte und stark verschmutzte Wäsche wird eine hohe Temperatur von 60-90°C empfohlen. Diese Temperatur\n","ermöglicht es dem Waschmittel, die hartnäckigen Flecken effektiv zu entfernen. Beachten Sie jedoch, dass diese\n","Temperatur nicht für alle Arten von Textilien geeignet ist, insbesondere für feine Materialien wie Wolle oder\n","Seide. In solchen Fällen sollten niedrigere Temperaturen verwendet werden.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 13\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 31\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 13\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Für helle Farben und extrem verschmutzte Wäsche ist die Verwendung von Flüssigwaschmitteln in der für stark\n","verschmutzte Kleidung empfohlenen Dosierung am besten. Diese Tipps helfen dir dabei, deine Wäsche effektiv zu\n","waschen und ihre Haltbarkeit zu verlängern.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 13\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 13\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 13\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 13\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Um die Wäsche zur Reinigung vorzubereiten, befolge diese Schritte:\n","\n","1. Öffne die Waschmaschinentür und lege die Wäschestücke los in die Maschine.\n","2. Drücke die Waschmaschinentür zu, bis sie hörbar einrastet, um sicherzustellen, dass nichts in der Tür\n","eingeklemmt ist.\n","3. Behalte während des Waschvorgangs die Waschmaschinentür und die Trommel offen, damit sich kein feuchter\n","Nährboden für Bakterien in der Maschine bilden kann.\n","4. Überlädere die Maschine nicht, da dies zu einer ungleichmäßigen Reinigung führen kann.\n","5. Verwende das passende Waschprogramm für Textilien, die mit \"maschinenwaschbar\" oder \"Handwäsche\"\n","gekennzeichnet sind.\n","6. Separiere bunte und weiße Textilien nicht, da sie möglicherweise beim Waschen stark abfärben können.\n","7. Behandle harte Verschmutzungen vor dem Waschen ordnungsgemäß.\n","8. Gib bei Bedarf zusätzliche Waschmittel hinzu, wie es im Waschprogramm angegeben ist.\n","9. Stelle sicher, dass du genügend Wasser für den Waschvorgang hast.\n","10. Beginne den Waschvorgang und warte auf das Ende des Programms, bevor du die Maschinetür öffnest.\n","\n","Denke daran, regelmäßig die Waschmaschinentür und die Trommel reinigen zu lassen, um eine saubere und gesunde\n","Maschine aufrechtzuerhalten.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 31\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 9\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 10\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 32\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Um Wäsche zu sortieren, befolge diese Schritte:\n","\n","1. Sortiere nach Farbe: Beginne mit den bunten Textilien und gib sie dann den weißen Textilien. Dies hilft\n","dabei, dass sich keine Farben vermischen und das Ergebnis ein sauberes und harmonisches Aussehen hat.\n","2. Sortiere nach Material: Textilien aus verschiedenen Materialien wie Baumwolle, Polyester, Nylon usw. haben\n","unterschiedliche Eigenschaften. Daher ist es wichtig, sie getrennt zu waschen, um Verbrennungs- oder\n","Verschmutzungsprobleme zu vermeiden.\n","3. Sortiere nach Größe: Groß- und kleinere Textilien sollten getrennt gewaschen werden, da große Textilien\n","leichter beschädigt werden können.\n","4. Sortiere nach Stoffqualität: Textilien aus verschiedenen Stoffen haben unterschiedliche Stoffqualitäten.\n","Daher ist es wichtig, hochwertige Textilien von weniger hochwertigen zu trennen, um ihre Haltbarkeit zu\n","erhalten.\n","5. Sortiere nach Bedarf: Textilien, die häufig benutzt werden, sollten vor Textilien, die seltener verwendet\n","werden, gewaschen werden.\n","6. Sortiere nach Reinheitsgrad: Textilien mit hohem Reinheitsgrad sollten vor Textilien mit niedrigem\n","Reinheitsgrad gewaschen werden, um Verunreinigungen zu vermeiden.\n","7. Sortiere nach Waschprogramm: Textilien, die verschiedene Waschprogramme erfordern, sollten entsprechend\n","sortiert werden. Zum Beispiel sollte Textilien mit einem hohen Feuchtigkeitsgehalt vor Textilien mit einem\n","niedrigen Feuchtigkeitsgehalt gewaschen werden.\n","8. Sortiere nach persönlichen Vorlieben: Es gibt auch individuelle Vorlieben bei der Sortierung von Wäsche. Du\n","kannst deine Lieblingstextilien oder solche, die du besonders wertschätzt, zuerst waschen.\n","\n","Indem du diesen Schritten folgst, kannst du Wäsche effektiv sortieren und so das Waschverfahren optimieren.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 29\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 9\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["df = pd.DataFrame.from_dict(results)"],"metadata":{"id":"NNA_m2--eLJL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"djZmLTZ6fC4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_excel(\"/content/drive/MyDrive/Colab Notebooks/\"+experiment_name+\".xlsx\")"],"metadata":{"id":"ti6MTwJcfKgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####### Random tests #############\n","query = \"Kann ich durch Betrachten der Waschmaschine einen Zustand tiefer innerer Entspannung erreichen?\"\n","response = pdf_qa({\"question\": query})\n","process_llm_response(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAS9jkeQjbO_","executionInfo":{"status":"ok","timestamp":1701081170453,"user_tz":-60,"elapsed":100395,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"110fafd0-d0c0-400b-c5fb-a5f7e32c1576"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" Es ist nicht empfohlen, die Waschmaschine als Mittel zur Erreichung eines Zustands tieferer innerer\n","Entspannung zu betrachten. Die Hauptfunktion einer Waschmaschine besteht darin, Kleidung zu waschen und zu\n","reinigen, und sie sollte nicht als Meditations- oder Entspannungstool verwendet werden. Stattdessen solltest\n","du andere Methoden wie Yoga, Meditation oder Atemübungen verwenden, um eine tiefe innere Ruhe zu erreichen.\n","Diese Techniken können dir helfen, deine Gedanken und Gefühle zu klären und dich auf eine ruhigeren und\n","entspanntere Weise zu fühlen.\n","\n","\n","Sources:\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 16\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 32\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 30\n","/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf 10\n"]},{"output_type":"execute_result","data":{"text/plain":["(' Es ist nicht empfohlen, die Waschmaschine als Mittel zur Erreichung eines Zustands tieferer innerer\\nEntspannung zu betrachten. Die Hauptfunktion einer Waschmaschine besteht darin, Kleidung zu waschen und zu\\nreinigen, und sie sollte nicht als Meditations- oder Entspannungstool verwendet werden. Stattdessen solltest\\ndu andere Methoden wie Yoga, Meditation oder Atemübungen verwenden, um eine tiefe innere Ruhe zu erreichen.\\nDiese Techniken können dir helfen, deine Gedanken und Gefühle zu klären und dich auf eine ruhigeren und\\nentspanntere Weise zu fühlen.',\n"," defaultdict(list,\n","             {'/content/drive/MyDrive/Colab Notebooks/machine_manual_truncated.pdf': [16,\n","               32,\n","               30,\n","               10]}))"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["memory.buffer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348},"id":"bDcSmOISVaRo","executionInfo":{"status":"ok","timestamp":1701081402859,"user_tz":-60,"elapsed":20,"user":{"displayName":"Muhammad Ahsan Ali","userId":"06941653628544840407"}},"outputId":"57ed8c99-6a5e-41dc-8b5e-bde2d60a35c3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nDie human fragt nach den Gefahren, die mit dieser Waschmaschine verbunden sind. Die KI gibt Informationen über potenzielle Gefahren mit folgenden Punkten an:\\n\\n1. Schäden an den Textilien: Unrichtige Einstellungen des Waschprogramms, unzureichende Reinigung der Trommel oder Verwendung von zu hoher Temperatur können zu Schäden an den Textilien führen. Dies kann zum Aufreißen von Stoffen, zur Verbrämung von Wollstoffen oder zum Abschaben von Farben führen.\\n2. Verstopfung der Trommel: Die Ansammlung von Rückständen aus dem Weichspüler, Waschmittel und anderen Verunreinigungen kann zu einer Verstopfung der Trommel führen. Dadurch kann die Waschmaschine nicht ordnungsgemäß funktionieren und die Reinigungseffektivität beeinträchtigt werden.\\n3. Brandgefahr: Die Verwendung von zu hoher Temperatur oder die Verwendung von unzureichendem Waschmittel kann zu einer Brandgefahr führen. Dies ist besonders wichtig bei elektrischen Heizgeräten, da sie leicht zu Bränden neigen können.\\n4. Elektroschockrisiko: Fehlkonnektion oder Defekte in der Waschmaschine können zu Stromschlägen oder Elektroschocks führen. Um dies zu vermeiden, solltest du immer einen geeigneten Schutzleiter verwenden und die Maschine regelmäßig überprüfen.\\n5. Unbeabsichtigte Veränderung der Textilien: Falsche Einstellungen des Waschprogramms oder die Verwendung von zu hohen Temperaturen können zu unbeabsichtigten Veränderungen der Textilien führen, z. B. Verbrämung von Wollstoffen oder Aufreißen von Stoffen.\\n6. Verschlechterung der Waschleistung: Eine verschmutzte Trommel kann die Waschleistung der Maschine beeinträchtigen und zu einer geringeren Reinigungseffektivität führen. Regelmäßige Reinigung der Trommel ist entsprechend wichtig.\\n7. Unangenehme Gerüche: Eine verschmutzte Trommel kann unangenehme Gerüche erzeugen, was zu unangenehmen Geruchssignalen in deiner Wohnung führt.\\n8. Reduzierte Lebensdauer: Eine unordnungsgemäß gewartete Waschmaschine kann ihre Lebensdauer reduzieren und zu häufigerem Reparaturbedarf führen.\\n9. Kann ich mit dem Wasser aus der Waschmaschine meine Katze waschen?\\nNein, es ist nicht empfehlenswert, das Wasser aus einer Waschmaschine für deine Katze zu verwenden. Obwohl das Wasser sauber sein könnte, enthält es often Chemikalien wie Detergenzien und Soften, die für Menschen unbedenklich sind, aber für Katzen giftig sein können. Darüber hinaus kann das Wasser aus einer Waschmaschine nicht auf die spezifischen Bedürfnisse und Empfindlichkeiten von Katzen zugeschnitten sein, was zu Vergiftungen führen oder andere Gesundheitsprobleme verursachen könnte. Es ist besser, für deine Katze geeignetes Trinkwasser oder Futterwasser zu verwenden. '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":[],"metadata":{"id":"VQDdpNWZVfTK"},"execution_count":null,"outputs":[]}]}